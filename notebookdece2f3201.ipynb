{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install faiss-gpu\n!pip install -U sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:09:32.047639Z","iopub.execute_input":"2025-01-23T05:09:32.047911Z","iopub.status.idle":"2025-01-23T05:09:42.313675Z","shell.execute_reply.started":"2025-01-23T05:09:32.047883Z","shell.execute_reply":"2025-01-23T05:09:42.312818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nimport pandas as pd\nimport urllib.request\nimport faiss\nimport time\nfrom sentence_transformers import SentenceTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:23:40.970431Z","iopub.execute_input":"2025-01-23T05:23:40.970734Z","iopub.status.idle":"2025-01-23T05:23:40.982307Z","shell.execute_reply.started":"2025-01-23T05:23:40.970704Z","shell.execute_reply":"2025-01-23T05:23:40.981481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:11:49.450073Z","iopub.execute_input":"2025-01-23T05:11:49.450375Z","iopub.status.idle":"2025-01-23T05:11:52.668556Z","shell.execute_reply.started":"2025-01-23T05:11:49.450351Z","shell.execute_reply":"2025-01-23T05:11:52.667691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/19.%20Topic%20Modeling%20(LDA%2C%20BERT-Based)/dataset/abcnews-date-text.csv\", filename=\"abcnews-date-text.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:13:00.068061Z","iopub.execute_input":"2025-01-23T05:13:00.068347Z","iopub.status.idle":"2025-01-23T05:13:00.390738Z","shell.execute_reply.started":"2025-01-23T05:13:00.068323Z","shell.execute_reply":"2025-01-23T05:13:00.389903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# CSV 파일 읽기\ndf = pd.read_csv(\"abcnews-date-text.csv\")\n\n# 'headline_text' 열을 리스트로 변환\ndata = df.headline_text.to_list()\n\n# 결과 확인\nprint(data[:5])  # 상위 5개 데이터 확인\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('총 샘플의 개수 :', len(data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:13:52.194527Z","iopub.execute_input":"2025-01-23T05:13:52.194839Z","iopub.status.idle":"2025-01-23T05:13:52.199718Z","shell.execute_reply.started":"2025-01-23T05:13:52.194811Z","shell.execute_reply":"2025-01-23T05:13:52.198803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:14:51.365656Z","iopub.execute_input":"2025-01-23T05:14:51.366006Z","iopub.status.idle":"2025-01-23T05:14:54.929227Z","shell.execute_reply.started":"2025-01-23T05:14:51.365978Z","shell.execute_reply":"2025-01-23T05:14:54.928143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('distilbert-base-nli-mean-tokens')\nencoded_data = model.encode(data)\nprint('임베딩 된 벡터 수 :', len(encoded_data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:15:28.270004Z","iopub.execute_input":"2025-01-23T05:15:28.270346Z","iopub.status.idle":"2025-01-23T05:23:40.922685Z","shell.execute_reply.started":"2025-01-23T05:15:28.270319Z","shell.execute_reply":"2025-01-23T05:23:40.921740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"index = faiss.IndexIDMap(faiss.IndexFlatIP(768))\nindex.add_with_ids(encoded_data, np.array(range(0, len(data))))\n\nfaiss.write_index(index, 'abc_news')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:23:40.983180Z","iopub.execute_input":"2025-01-23T05:23:40.983451Z","iopub.status.idle":"2025-01-23T05:23:47.495567Z","shell.execute_reply.started":"2025-01-23T05:23:40.983403Z","shell.execute_reply":"2025-01-23T05:23:47.494628Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 이번 프로젝트 화장품 성분 임배딩 텍스트 관리 DB","metadata":{}},{"cell_type":"code","source":"def search(query):\n   t = time.time()\n   query_vector = model.encode([query])\n   k = 5\n   top_k = index.search(query_vector, k)\n   print('total time: {}'.format(time.time() - t))\n   return [data[_id] for _id in top_k[1].tolist()[0]]\nquery = str(input())\nresults = search(query)\n\nprint('results :')\nfor result in results:\n   print('\\t', result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:24:57.910898Z","iopub.execute_input":"2025-01-23T05:24:57.911247Z","iopub.status.idle":"2025-01-23T05:25:12.603104Z","shell.execute_reply.started":"2025-01-23T05:24:57.911220Z","shell.execute_reply":"2025-01-23T05:25:12.602182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}